---
layout: post
title:  "pod网络 & dns"
date:   2018-05-23
categories: [k8s]
---

# pod网络 & dns

## 测试pod网络和dns
用busyboxlpus：curl测试pod网络和dns。

```bash
kubectl run -it busycurl --image=docker.io/radial/busyboxplus:curl
kubectl run -it busycurl01 --image=docker.io/radial/busyboxplus:curl

#创建了两个pod（pod中只含一个容器）
$ kubectl get po
NAME                          READY     STATUS    RESTARTS   AGE
busycurl-599db6679b-mw7qs     1/1       Running   2          4h
busycurl01-7754c76868-c5g64   1/1       Running   1          4h

#进入pod，尝试ping 同ns下的其它pod，成功
kubectl attach -it busycurl-599db6679b-mw7qs
```


测试dns：

```bash
#解析同ns下的pod，成功
[ root@busycurl01-7754c76868-c5g64:/ ]$ nslookup kubernetes.default
Server:    10.96.0.10
Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local

Name:      kubernetes.default
Address 1: 10.96.0.1 kubernetes.default.svc.cluster.local

#解析不同ns下的pod，成功
[ root@busycurl01-7754c76868-c5g64:/ ]$ nslookup kubernetes-dashboard.kube-system
Server:    10.96.0.10
Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local

Name:      kubernetes-dashboard.kube-system
Address 1: 10.100.120.172 kubernetes-dashboard.kube-system.svc.cluster.local

#尝试ping不同ns 的 IP，失败
[ root@busycurl01-7754c76868-c5g64:/ ]$ ping 10.100.120.172
PING 10.100.120.172 (10.100.120.172): 56 data bytes
^C
--- 10.100.120.172 ping statistics ---
10 packets transmitted, 0 packets received, 100% packet loss
```

## 计算机网络fundamental

### PING一个IP时发生了什么
ping ip X 时，会发送一个ICMP echo 请求包。
1. 在LAN下寻找IP X
2. 如找到（肯能通过转发？）则发送ping包。其header包括：ICMP、IP（已知Source IP 和 Dst IP）、Ethernet（已知Source MAC，但不知道 Dst MAC）
3. 通过ARP（地址解析协议）找到MAC，发包

* 互联网控制消息协议（英语：Internet Control Message Protocol，缩写：ICMP）是互联网协议族的核心协议之一。 它用于TCP/IP网络中发送控制消息，提供可能发生在通信环境中的各种问题反馈，通过这些信息，使管理者可以对所发生的问题作出诊断，然后采取适当的措施解决。



# Pod Definition

## Pod management

## Volume 
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: redis
spec:
  containers:
  - name: redis
    image: redis
    volumeMounts:
    - name: redis-persistent-storage
      mountPath: /data/redis    #挂在到容器的目录下
  volumes:
  - name: redis-persistent-storage
    emptyDir: {}    #再pod运行时创建一个新目录，该目录可以在容器重启后依然 persist
```

## Labels, Deployments, Services and Health Checking

### labels

```yaml
#pod-nginx-with-label.yaml docs/user-guide/walkthrough  
apiVersion: v1
kind: Pod
metadata:
  name: nginx
  labels:
    app: nginx
spec:
  containers:
  - name: nginx
    image: nginx
    ports:
    - containerPort: 80
```
List all Pods with the label app=nginx:
```bash
kubectl get pods -l app=nginx
```

Delete the Pod by label:
```bash
kubectl delete pod -l app=nginx
```
### Deployment
deploy的定义是建立在pod之上的，为了解决这些问题：

当容器越来越多的时候，如何scale pod的数量？如何roll out新版本？

deployment是为了管理、维护和升级正在运行的pod而抽象出来的。--pod creation template（用label） & 期望的pod副本数。
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  selector:
    matchLabels:
      app: nginx
  replicas: 2 # tells deployment to run 2 pods matching the template
  #模版生命了一个完整的pod
  template: # create pods using pod definition in this template
    metadata:
      # unlike pod-nginx.yaml, the name is not included in the meta data as a unique name is generated from the deployment name
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.7.9
        ports:
        - containerPort: 80
```


### Service
把不同的任务分割开，令其互相合作又互不影响。

```yaml
apiVersion: v1
kind: Service
metadata:
  name: nginx-service
spec:
  ports:
  - port: 8000 # the port that this service should serve on
    # the container on each pod to connect to, can be a name
    # (e.g. 'www') or a number (e.g. 80)
    targetPort: 80
    protocol: TCP
  # just like the selector in the deployment,
  # but this time it identifies the set of pods to load balance
  # traffic to.
  selector:
    app: nginx
```

#### 通信的问题
此处发现，pod中甚至 nslookup google.com 都没问题。然而服务的VIP不在pod网段内，所以无法ping通。

看来pod的扁平网络指的仅仅是pod，而非对于k8s的所有抽象类型网络都是扁平的。

!! 看起来是所有能ping的ip都是 end point，不能ping的是没有end point的（服务没有pod、）。

```bash
[ root@busycurl-599db6679b-mw7qs:/ ]$ nslookup google.com
Server:    10.96.0.10
Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local

Name:      google.com
Address 1: 2404:6800:4008:803::200e tsa03s02-in-x0e.1e100.net
Address 2: 216.58.203.14 hkg12s09-in-f14.1e100.net

[ root@busycurl-599db6679b-mw7qs:/ ]$ nslookup nginx-service.default
Server:    10.96.0.10
Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local

Name:      nginx-service.default
Address 1: 10.98.40.221 nginx-service.default.svc.cluster.local

[ root@busycurl-599db6679b-mw7qs:/ ]$ ip addr
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
3: eth0@if14: <BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> mtu 1450 qdisc noqueue
    link/ether 0a:58:0a:f4:01:17 brd ff:ff:ff:ff:ff:ff
    inet 10.244.1.23/24 scope global eth0
       valid_lft forever preferred_lft forever
```

# IP和VIP
* Pod IP 实际指向一个固定的地址，SVC 的 IP 背后并没有一个实际的主机在应答（也就么有MAC？那么IP和VIP的区别可以暂时简单的理解为有没有MAC？）。
* 我们使用iptables（Linux中的数据包处理逻辑）来定义虚拟IP地址，并根据需要进行透明重定向（请求连接到VIP时会自动转发到endpoint）
* 服务的环境变量和DNS 实际上被服务的VIP和端口来填充

# 调试k8s的服务
部署完服务之后，还需要进一步检查：
* Service 存在吗？
* Service 是否通过 DNS 工作？
* DNS 中是否存在服务？
* Service 是通过 IP 工作的吗？
* Service 是对的吗？
* Service 有 Endpoints 吗？
* Pod 工作正常吗？- curl pod, 查看重启次数
* kube-proxy 工作正常吗？

## 检查 Service 配置是否正确

* spec.ports[] 中描述的是您想要尝试访问的端口吗？
* targetPort 对您的 Pod 来说正确吗（许多 Pod 选择使用与 Service 不同的端口）？
* 如果您想把它变成一个数字端口，那么它是一个数字（8000）还是字符串 “8000”？
* 如果您想把它当作一个指定的端口，那么您的 Pod 是否公开了一个同名端口？
* 端口的 protocol 和 Pod 的一样吗？

```json
$ kubectl get svc nginx-service -o json
{
    "apiVersion": "v1",
    "kind": "Service",
    "metadata": {
        "annotations": {
            "kubectl.kubernetes.io/last-applied-configuration": "{\"apiVersion\":\"v1\",\"kind\":\"Service\",\"metadata\":{\"annotations\":{},\"name\":\"nginx-service\",\"namespace\":\"default\"},\"spec\":{\"ports\":[{\"port\":8000,\"protocol\":\"TCP\",\"targetPort\":80}],\"selector\":{\"app\":\"nginx\"}}}\n"
        },
        "creationTimestamp": "2018-05-09T11:51:44Z",
        "name": "nginx-service",
        "namespace": "default",
        "resourceVersion": "227367",
        "selfLink": "/api/v1/namespaces/default/services/nginx-service",
        "uid": "585b6b66-537f-11e8-8a96-1c872ccbdeb2"
    },
    "spec": {
        "clusterIP": "10.98.40.221",
        "ports": [
            {
                "port": 8000,
                "protocol": "TCP",
                "targetPort": 80
            }
        ],
        "selector": {
            "app": "nginx"
        },
        "sessionAffinity": "None",
        "type": "ClusterIP"
    },
    "status": {
        "loadBalancer": {}
    }
}
```
## 服务有没有 endpoint

```bash
$ kubectl get po -l app=nginx
NAME                                READY     STATUS    RESTARTS   AGE
nginx-deployment-7694d679d9-fs8tq   1/1       Running   0          53m
nginx-deployment-7694d679d9-hngx9   1/1       Running   0          53m

# -l app=hostnames 参数是一个标签选择器 - 就像我们的 Service 一样。
# 在 Kubernetes 系统中有一个控制循环，它评估每个 Service 的选择器，
# 并将结果保存到 Endpoints 对象中。

#查看对应的 endpoint
$ kubectl get endpoints nginx-service
NAME            ENDPOINTS                       AGE
nginx-service   10.244.1.36:80,10.244.1.37:80   15h
```
上面的结果证实 endpoints 控制器已经为您的 Service 找到了正确的 Pods。如果 nginx-service 行为空，则应检查 Service 的 spec.selector 字段，以及您实际想选择的 Pods 的 metadata.labels 的值。常见的错误是设置出现了问题，例如 Service 想选择 run=nginx-service，但是 Deployment 指定的是 app=nginx-service。

## proxy 是否正在写 iptables 规则?

kube-proxy 的主要职责之一是写实现 Services 的 iptables 规则。让我们检查一下这些规则是否已经被写好了。

kube-proxy 可以在 “userspace” 模式或 “iptables” 模式下运行。希望您正在使用更新、更快、更稳定的 “iptables” 模式。您应该看到以下情况:


```bash
 sudo iptables-save | grep nginx-service
-A KUBE-SEP-2DGLPGXWBWWNXDYH -s 10.244.1.37/32 -m comment --comment "default/nginx-service:" -j KUBE-MARK-MASQ
-A KUBE-SEP-2DGLPGXWBWWNXDYH -p tcp -m comment --comment "default/nginx-service:" -m tcp -j DNAT --to-destination 10.244.1.37:80
-A KUBE-SEP-Y2LY4L3MZQNIXQHE -s 10.244.1.36/32 -m comment --comment "default/nginx-service:" -j KUBE-MARK-MASQ
-A KUBE-SEP-Y2LY4L3MZQNIXQHE -p tcp -m comment --comment "default/nginx-service:" -m tcp -j DNAT --to-destination 10.244.1.36:80
-A KUBE-SERVICES ! -s 10.244.0.0/16 -d 10.98.40.221/32 -p tcp -m comment --comment "default/nginx-service: cluster IP" -m tcp --dport 8000 -j KUBE-MARK-MASQ
-A KUBE-SERVICES -d 10.98.40.221/32 -p tcp -m comment --comment "default/nginx-service: cluster IP" -m tcp --dport 8000 -j KUBE-SVC-GKN7Y2BSGW4NJTYL
-A KUBE-SVC-GKN7Y2BSGW4NJTYL -m comment --comment "default/nginx-service:" -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-Y2LY4L3MZQNIXQHE
-A KUBE-SVC-GKN7Y2BSGW4NJTYL -m comment --comment "default/nginx-service:" -j KUBE-SEP-2DGLPGXWBWWNXDYH
```
* KUBE-SERVICES 中应该有 1 条规则
* KUBE-SVC-(hash) 中每个 endpoint 有 1 或 2 条规则（取决于 SessionAffinity）
* 每个 endpoint 中应有 1 条 KUBE-SEP-(hash) 链。准确的规则将根据您的确切配置（包括 节点-端口 以及 负载均衡）而有所不同。
