---
layout: post
title:  "在k8s上调度GPU"
date:   2018-05-23
categories: [Docker]
---

# 在k8s上调度GPU

戳[这里](https://kubernetes.io/cn/docs/tasks/manage-gpus/scheduling-gpus/)查看官方文档

## 方法一(alpha,将在1.11版本移除)

```bash
#获取所有nginx pod的ip
kubectl get po -l app=nginx -o jsonpath='{.items[1].status.podIP}'
```

Since we want to build a cluster that uses GPUs we need to enable GPU acceleration in the master node. Keep in mind, that this instruction may become obsolete or change completely in a later version of Kubernetes!

Add GPU support to the Kubeadm configuration, while cluster is not initialized.
```bash
sudo vim /etc/systemd/system/kubelet.service.d/10-kubeadm.conf

#append ExecStart with the flag —feature-gates="Accelerators=true", so it will look like this:
ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS [...] --feature-gates="Accelerators=true"

#Restart kubelet
sudo systemctl daemon-reload
sudo systemctl restart kubelet
```

## 方法二：Device 插件
Kubernetes v1.8 开始增加了 Alpha 版的 Device 插件，用来支持 GPU、FPGA、高性能 NIC、InfiniBand 等各种设备。这样，设备厂商只需要根据 Device Plugin 的接口实现一个特定设备的插件，而不需要修改 Kubernetes 核心代码。
在 v1.10 中该特性升级为 Beta 版本。

### Device 插件原理
使用 Device 插件之前，首先要开启 DevicePlugins 功能，即配置 --feature-gates=DevicePlugins=true（默认是关闭的）。
Device 插件实际上是一个 gPRC 接口，需要实现 ListAndWatch() 和 Allocate() 等方法，并监听 gRPC Server 的 Unix Socket 在 /var/lib/kubelet/device-plugins/ 目录中，如 /var/lib/kubelet/device-plugins/nvidiaGPU.sock。
在实现 Device 插件时需要注意:
* 插件启动时，需要通过 /var/lib/kubelet/device-plugins/kubelet.sock 向 Kubelet 注册，同时提供插件的 Unix Socket 名称、API 的版本号和插件名称（格式为 vendor-domain/resource，如 nvidia.com/gpu）。Kubelet 会将这些设备暴露到 Node 状态中，方便后续调度器使用
* 插件启动后向 Kubelet 发送插件列表、按需分配设备并持续监控设备的实时状态
* 插件启动后要持续监控 Kubelet 的状态，并在 Kubelet 重启后重新注册自己。比如，Kubelet 刚启动后会清空 /var/lib/kubelet/device-plugins/ 目录，所以插件作者可以监控自己监听的 unix socket 是否被删除了，并根据此事件重新注册自己.

![yo](https://wx4.sinaimg.cn/mw1024/0078IDjtgy1frd8lxn194j30o90hymy1.jpg)

Device 插件一般推荐使用 DaemonSet 的方式部署，并将 /var/lib/kubelet/device-plugins 以 Volume 的形式挂载到容器中。当然，也可以手动运行的方式来部署，但这样就没有失败自动恢复的功能了。

```bash
# For Kubernetes v1.9
kubectl create -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/v1.9/nvidia-device-plugin.yml
```

## 配置GPU node

通过安装插件，此时kubernetes应该已经可以识别宿主机上的nvidia.com/gpu资源对象。


```bash
#本地运行gpu docker
sudo docker run --runtime=nvidia --network host --rm  -it -p 8888:8888 gcr.io/tensorflow/tensorflow:latest-gpu

kubectl run tf-pod --image=192.168.1.133:5000/srf_with_anaconda:v2.11 --limits=nvidia.com/gpu=1 -i --tty

#退出后可以重新进入：
kubectl attach tf-pod-0-74b945d7f5-5j5b9 -c tf-pod-0 -i -t
```

## note
在参考[这篇文章](http://kubernetesbyexample.com/sd/)测试service时，遇到了 no route to host 错误。参考[这个](https://github.com/rancher/rancher/issues/6139)方法删除了所有iptables规则。所造成的影响尚不清楚。
