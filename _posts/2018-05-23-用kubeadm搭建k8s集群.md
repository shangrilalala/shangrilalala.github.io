---
layout: post
title:  "用kubeadm搭建k8s集群"
date:   2018-05-23
categories: [k8s]
---

# k8s学习笔记（用kubeadm搭建k8s集群）

![yo](https://wx2.sinaimg.cn/mw1024/0078IDjtgy1fr0fcwddd3j30b40b43yz.jpg)

## 1. k8s的容器编排逻辑

k8s对于容器的编排方式是设定一个预期的容器组的状态，并维护集群使其一直处于这一状态。进行这项工作的就是 kubernetes API，容器组建立起来，它会实例化一个kubernetes API对象去负责持续监控：

* 要run什么app或workload
* 用什么容器镜像，启动多少复本
* 用什么网络/disk源

然后kube control panel会向设定的状态转化。（开启容器，scaling）

## 2. 宿主机系统准备

* 修改主机名（最好为 xxx.xxx 这种域名格式）
1. 在CentOS 7中，有个叫hostnamectl的命令行工具，它允许你查看或修改与主机名相关的配置。
2. hostnamectl set-hostname <hostname>
3. 手动更新/etc/hosts
4. reboot -f

* 在各节点禁用防火墙 和 SELinux
```bash
#禁用防火墙
systemctl stop firewalld
systemctl disable firewalld

#禁用SELinux
setenforce 0
vi /etc/selinux/config
SELINUX=disabled

#创建/etc/sysctl.d/k8s.conf文件，添加如下内容：
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
sudo sysctl --system
```

* 禁用磁盘交换分区
```bash
#查找TYPE=”swap”行以确定交换分区
$ qinglong@BME311-Node3:~$ blkid
/dev/sda3: UUID="c8179caf-3f9c-44e2-ae38-987a60e46982" TYPE="swap" PARTUUID="52012b85-1600-4e6e-9bbd-3806f84d17fd"
/dev/sda1: UUID="5501-B48B" TYPE="vfat" PARTLABEL="EFI System Partition" PARTUUID="b4b5017e-8524-4d5e-92c2-71e4766b98f4"
/dev/sda2: UUID="a49c4bc3-11c0-404f-9905-ce63d7cea5c8" TYPE="ext4" PARTUUID="972e3425-89ed-40dc-afb0-6f7c1103bec6"

#搜索和识别[SWAP]分区
$ qinglong@BME311-Node3:~$ lsblk
NAME   MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
sr0     11:0    1 1024M  0 rom
sda      8:0    0  1.8T  0 disk
├─sda2   8:2    0  1.8T  0 part /
├─sda3   8:3    0   64G  0 part [SWAP]
└─sda1   8:1    0  512M  0 part /boot/efi

#禁用所有交换
$ sudo swapoff -a 
#检查是否禁用成功
$ free -h
#注释掉交换分区配置行,以永久禁用
$ sudo vim /etc/fstab

#添加Google yum gpg并安装kubeadm,kubelet and kubectl
cat <<EOF > /etc/yum.repos.d/kubernetes.repo 
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg   https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
EOF 
setenforce 0 
yum install -y kubelet kubeadm kubectl systemctl enable kubelet && systemctl start kubelet
```


### 2.1 k8s集群初始化工具

* kubeadm-引导（bootstrap）集群构建的工具
    * kubead不会管理kubectl和kubelet，仅在集群初始化时对集群进行配置；
    * 此处需要注意kubeadm的版本要高于kubelet和kubectl。
* kubectl-k8s的CLI
* kubelet-监控pod和容器的工具，提供健康检查，集群的神经系统

## 3. 集群启动引导

### 3.1 安装初始化工具（centos 和 ubuntu）
```bash
#这里使用1.9.6版本

#ubuntu
sudo apt-get install kubeadm=1.9.6-00 kubelet=1.9.6-00 kubectl=1.9.6-00

#centos
yum install -y kubelet-1.9.6-0.x86_64 kubeadm-1.9.6-0.x86_64 kubectl-1.9.6-0.x86_64
```

### 3.2 Configure cgroup driver used by kubelet on Master Node
Make sure that the cgroup driver used by kubelet is the same as the one used by Docker. Verify that your Docker cgroup driver matches the kubelet config:

```bash
docker info | grep -i cgroup
cat /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
```
把kubelet的cgroup调成与docker相同，否则kubelet会无法正常运行！

更改之后，重启kubelet进程：
```bash
systemctl daemon-reload
systemctl restart kubelet
```

### 3.3 启动集群

使用命令：
```bash
kubeadm init --kubernetes-version=v1.9.6 --apiserver-advertise-address=192.168.1.129 --pod-network-cidr=10.244.0.0/16
```

其中
* v1.9.6指定了kubernetes的版本
* --apiserver-advertise-address=192.168.1.129 是master node 的IP地址
* --pod-network-cidr=10.244.0.0/16 因为我们选择flannel作为Pod网络插件


```bash
# qinglong @ hqkube-master-zeng in ~ [15:33:29]
$ sudo kubeadm init --kubernetes-version=v1.9.6 --apiserver-advertise-address=192.168.1.185 --pod-network-cidr=10.244.0.0/16
[init] Using Kubernetes version: v1.9.6
[init] Using Authorization modes: [Node RBAC]
[preflight] Running pre-flight checks.
	[WARNING SystemVerification]: docker version is greater than the most recently validated version. Docker version: 18.03.0-ce. Max validated version: 17.03
	[WARNING FileExisting-crictl]: crictl not found in system path
[preflight] Starting the kubelet service
[certificates] Generated ca certificate and key.
[certificates] Generated apiserver certificate and key.
[certificates] apiserver serving cert is signed for DNS names [hqkube-master-zeng.master kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.1.129]
[certificates] Generated apiserver-kubelet-client certificate and key.
[certificates] Generated sa key and public key.
[certificates] Generated front-proxy-ca certificate and key.
[certificates] Generated front-proxy-client certificate and key.
[certificates] Valid certificates and keys now exist in "/etc/kubernetes/pki"
[kubeconfig] Wrote KubeConfig file to disk: "admin.conf"
[kubeconfig] Wrote KubeConfig file to disk: "kubelet.conf"
[kubeconfig] Wrote KubeConfig file to disk: "controller-manager.conf"
[kubeconfig] Wrote KubeConfig file to disk: "scheduler.conf"
[controlplane] Wrote Static Pod manifest for component kube-apiserver to "/etc/kubernetes/manifests/kube-apiserver.yaml"
[controlplane] Wrote Static Pod manifest for component kube-controller-manager to "/etc/kubernetes/manifests/kube-controller-manager.yaml"
[controlplane] Wrote Static Pod manifest for component kube-scheduler to "/etc/kubernetes/manifests/kube-scheduler.yaml"
[etcd] Wrote Static Pod manifest for a local etcd instance to "/etc/kubernetes/manifests/etcd.yaml"
[init] Waiting for the kubelet to boot up the control plane as Static Pods from directory "/etc/kubernetes/manifests".
[init] This might take a minute or longer if the control plane images have to be pulled.
[apiclient] All control plane components are healthy after 26.001057 seconds
[uploadconfig] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[markmaster] Will mark node hqkube-master-zeng.master as master by adding a label and a taint
[markmaster] Master hqkube-master-zeng.master tainted and labelled with key/value: node-role.kubernetes.io/master=""
[bootstraptoken] Using token: 6fa700.5204f30369f2ed39
[bootstraptoken] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstraptoken] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstraptoken] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstraptoken] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
[addons] Applied essential addon: kube-dns
[addons] Applied essential addon: kube-proxy

Your Kubernetes master has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of machines by running the following on each node
as root:

 sudo kubeadm join --token ff511d.c81887d6a8020b52 192.168.1.139:6443 --discovery-token-ca-cert-hash sha256:21bd674a4fc4a993a5ab98043b4507beb8376e993aadd33653104b536042bc84
```

### 3.4 为集群配置CNI插件
Note:

* For flannel to work correctly, --pod-network-cidr=10.244.0.0/16 has to be passed to kubeadm init.
* flannel works on amd64, arm, arm64 and ppc64le, but for it to work on a platform other than amd64 you have to manually download the manifest and replace amd64 occurrences with your chosen platform.
* Set /proc/sys/net/bridge/bridge-nf-call-iptables to 1 by running sysctl net.bridge.bridge-nf-call-iptables=1 to pass bridged IPv4 traffic to iptables’ chains. This is a requirement for some CNI plugins to work, for more information please see here.
```bash
#使用配置文件安装flannel
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/v0.9.1/Documentation/kube-flannel.yml
```

### 3.5 加入新节点
k8s回默认24h之后token过期。可以这样解决：
```bash
$ kubeadm token list
$ kubeadm token create
$ sudo kubeadm token create --print-join-command
```

```bash
# hqkubeadmin @ hqkube-node-ws7 in ~ [3:53:59] C:2
$ sudo kubeadm join --token 6fa700.5204f30369f2ed39 192.168.1.129:6443 --discovery-token-ca-cert-hash sha256:88e32bf86fbab80a17495d52abad3677ce9845e5da8a275476b5c6413964d318
[sudo] hqkubeadmin 的密码：
[preflight] Running pre-flight checks.
	[WARNING FileExisting-crictl]: crictl not found in system path
[preflight] Starting the kubelet service
[discovery] Trying to connect to API Server "192.168.1.129:6443"
[discovery] Created cluster-info discovery client, requesting info from "https://192.168.1.129:6443"
[discovery] Requesting info from "https://192.168.1.129:6443" again to validate TLS against the pinned public key
[discovery] Cluster info signature and contents are valid and TLS certificate validates against pinned roots, will use API Server "192.168.1.129:6443"
[discovery] Successfully established connection with API Server "192.168.1.129:6443"

This node has joined the cluster:
* Certificate signing request was sent to master and a response
  was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the master to see this node join the cluster.
```

### 3.6 测试DNS
```bash
# qinglong @ hqkube-master-zeng in ~ [16:06:02]
$ kubectl run curl --image=radial/busyboxplus:curl -i --tty
If you don't see a command prompt, try pressing enter.
[ root@curl-545bbf5f9c-2pzsc:/ ]$ ls
bin/     dev/     etc/     home/    lib/     lib64    linuxrc  media/   mnt/     opt/     proc/    root/    run      sbin/    sys/     tmp/     usr/     var/
[ root@curl-545bbf5f9c-2pzsc:/ ]$ exit
Session ended, resume using 'kubectl attach curl-545bbf5f9c-2pzsc -c curl -i -t' command when the pod is running
```
进入刚刚运行的pod内，运行nslookup kubernetes.default。

以下输出证明cluster内dns能够解析正常。
```bash
# qinglong @ hqkube-master-zeng in ~ [16:10:32] C:1
$ kubectl exec curl-545bbf5f9c-2pzsc nslookup kubernetes.default
Server:    10.96.0.10
Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local

Name:      kubernetes.default
Address 1: 10.96.0.1 kubernetes.default.svc.cluster.local
```

## 4. k8s dasnboard设置

```bash
#部署dashboard
$ kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml
#开启proxy服务，才能访问dashboard
$ kubectl proxy
```

kubectl proxy creates proxy server between your machine and Kubernetes API server. By default it is only accessible locally (from the machine that started it).

默认配置下，dashboard的服务ip类行为clusterIP，只能通过[本地链接](http://127.0.0.1:8001/ui)访问。

```bash
# qinglong @ hqkube-master-zeng in ~ [11:19:24]
$ kubectl cluster-info
Kubernetes master is running at https://192.168.1.129:6443
KubeDNS is running at https://192.168.1.129:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy

To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.
```

### 4.1 首次登陆认证
Getting token with kubectl

There are many Service Accounts created in Kubernetes by default. All with different access permissions. In order to find any token, that can be used to log in we'll use kubectl:

查看集群服务中现有的token，我们选择带有一定权限的token登陆。

```bash
# qinglong @ hqkube-master-zeng in ~ [16:25:12]
$ kubectl -n kube-system get secret
NAME                                             TYPE                                  DATA      AGE
attachdetach-controller-token-q9hl6              kubernetes.io/service-account-token   3         51m
bootstrap-signer-token-9xfqw                     kubernetes.io/service-account-token   3         51m
bootstrap-token-6fa700                           bootstrap.kubernetes.io/token         7         51m
certificate-controller-token-h2l7f               kubernetes.io/service-account-token   3         51m
clusterrole-aggregation-controller-token-crj66   kubernetes.io/service-account-token   3         51m
cronjob-controller-token-bf8tq                   kubernetes.io/service-account-token   3         50m
daemon-set-controller-token-psl28                kubernetes.io/service-account-token   3         51m
default-token-k769p                              kubernetes.io/service-account-token   3         50m
deployment-controller-token-25z45                kubernetes.io/service-account-token   3         50m
disruption-controller-token-nk5zn                kubernetes.io/service-account-token   3         50m
endpoint-controller-token-rkvcz                  kubernetes.io/service-account-token   3         50m
flannel-token-67lw8                              kubernetes.io/service-account-token   3         29m
generic-garbage-collector-token-znkhm            kubernetes.io/service-account-token   3         50m
horizontal-pod-autoscaler-token-vgjtg            kubernetes.io/service-account-token   3         51m
job-controller-token-zp4b5                       kubernetes.io/service-account-token   3         51m
kube-dns-token-kt8wh                             kubernetes.io/service-account-token   3         51m
kube-proxy-token-kq6cf                           kubernetes.io/service-account-token   3         51m
kubernetes-dashboard-certs                       Opaque                                0         1m
kubernetes-dashboard-key-holder                  Opaque                                2         1m
kubernetes-dashboard-token-4xcff                 kubernetes.io/service-account-token   3         1m
namespace-controller-token-p9jvd                 kubernetes.io/service-account-token   3         50m
node-controller-token-dwc8k                      kubernetes.io/service-account-token   3         51m
persistent-volume-binder-token-b296b             kubernetes.io/service-account-token   3         50m
pod-garbage-collector-token-g6qqv                kubernetes.io/service-account-token   3         51m
pv-protection-controller-token-dslsz             kubernetes.io/service-account-token   3         50m
pvc-protection-controller-token-tw8vx            kubernetes.io/service-account-token   3         50m
replicaset-controller-token-t888v                kubernetes.io/service-account-token   3         50m
replication-controller-token-gtrh4               kubernetes.io/service-account-token   3         51m
resourcequota-controller-token-4kg6j             kubernetes.io/service-account-token   3         50m
service-account-controller-token-m9k87           kubernetes.io/service-account-token   3         51m
service-controller-token-jbw2t                   kubernetes.io/service-account-token   3         50m
statefulset-controller-token-4gtx4               kubernetes.io/service-account-token   3         50m
token-cleaner-token-kbmg6                        kubernetes.io/service-account-token   3         51m
ttl-controller-token-679nm                       kubernetes.io/service-account-token   3         50m

# Let's get token from 'replication-controller-token-gtrh4'. 
# It should have permissions to see Replica Sets in the cluster.

# qinglong @ hqkube-master-zeng in ~ [16:25:21]
$ kubectl describe secret -n kube-system replication-controller-token-gtrh4
Name:         replication-controller-token-gtrh4
Namespace:    kube-system
Labels:       <none>
Annotations:  kubernetes.io/service-account.name=replication-controller
              kubernetes.io/service-account.uid=0ecf21d2-485b-11e8-a3b3-1c872ccbde29

Type:  kubernetes.io/service-account-token

Data
====
ca.crt:     1025 bytes
namespace:  11 bytes
token:      eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJyZXBsaWNhdGlvbi1jb250cm9sbGVyLXRva2VuLWd0cmg0Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6InJlcGxpY2F0aW9uLWNvbnRyb2xsZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiIwZWNmMjFkMi00ODViLTExZTgtYTNiMy0xYzg3MmNjYmRlMjkiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06cmVwbGljYXRpb24tY29udHJvbGxlciJ9.aNuuQuADPLacNNyIDtts-xNT1RCCxRh-1q-EmJv43c4kgJuLOXz0FLlG98MhR1i2re_boGLgzzIlLuNCEQHw5fhYXLGdLgn-LVG3fnupEHlLU3hw_lwCA22ZSCT9SAeX-fCFCtc8jHg70QgR1km2Uig1q17eebnWBnYyx_42W1ts0GH_Q996vyphUCdTamiOd_XyxlkTIgZWNwVhStkDzvKQNLeck98xPpcNWad_GPbvVCFtDC1Wi7_jc1lHQ0W3dP_G1oQWNqAgtlO7ekAp9PPaUcge26QEjq_8UJ0ymdQtF0QrASGGZRLk0HArKFo_PCFS8p1afZggnWDlAIPjtg
```

此方法等同于修改admin.conf文件（向其中添加token），都是利用已有的token访问。此时可以登录了。但是这个token的权限很低，大部分内容并不能查看。

经过调研，在[这里](https://segmentfault.com/a/1190000013681047)找到了有**完全权限的token**。
```bash
# qinglong @ hqkube-master-zeng in ~ [16:26:27]
$ kubectl -n kube-system describe $(kubectl -n kube-system get secret -n kube-system -o name | grep namespace) | grep token
Name:         namespace-controller-token-p9jvd
Type:  kubernetes.io/service-account-token
token:      
eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJuYW1lc3BhY2UtY29udHJvbGxlci10b2tlbi1oZzluNiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJuYW1lc3BhY2UtY29udHJvbGxlciIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6ImZmODQ2NTIyLTUxYmUtMTFlOC1iODAyLTFjODcyY2NiZGViMiIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlLXN5c3RlbTpuYW1lc3BhY2UtY29udHJvbGxlciJ9.sYwagVfzRJ3N8EStoiQKKzxY8EgTj4V_EvwClTRe5tPqCwwSubEXQxOHUp2P6kMAFOIE9V0CnSrVGj_u5UHX2WMMrOmexLLmybq02lpRBH6arWj6bqKbKM2vAdinOq3mNo9GOrdKn4RuiDfqm3Ke_nm-6gu-oWy7Fg94dj4AvRoOyzaHhxYbCVjdrvRw9ooFLWF-x1QWKxFgl3sadQZc-Sx1hvkJX21xBHYZFTijowdw4AblBUGg_AYPQdOM7Ja8I2jF71YqBJAydJE31FKHfx9JyiulSSTeI9CsSo1PObrSo18RUUAKKc9oEsuYFOTpUo3b4tVNkPrbqL1tN4OjOQ
```

也可以手动查询：
```bash
# 输入下面命令查询kube-system命名空间下的所有secret
kubectl get secret -n kube-system

# 然后获取token。只要是type为service-account-token的secret的token都可以使用。
# 比如我们获取replicaset-controller-token-wsv4v的touken
kubectl -n kube-system describe replicaset-controller-token-wsv4v
```

更好的解决方案应该是可以建立用户，并对用户分配权限。关于实现具体解释在[Github dashboard issue #2474](https://github.com/kubernetes/dashboard/issues/2474) 和 [Dashboard wiki-Creating sample user](https://github.com/kubernetes/dashboard/wiki/Creating-sample-user)。


>I ran **kubectl get clusterRoles** and **kubectl get clusterRoleBindings** and saw an item called **cluster-admin** in both. However unlike other role bindings (e.g. tiller-cluster-rule), the cluster-admin one referred to something called apiGroup instead of ServiceAccount (to which a token can belong). Check out the difference in the bottom of each output:
```bash
# qinglong @ hqkube-master-zeng in /etc/kubernetes [13:17:45] C:1
$ kubectl get clusterRoles
NAME                                                                   AGE
admin                                                                  16h
cluster-admin                                                          16h
edit                                                                   16h
flannel                                                                3h
system:aggregate-to-admin                                              16h
...
system:node-bootstrapper                                               16h
system:node-problem-detector                                           16h
system:node-proxier                                                    16h
system:persistent-volume-provisioner                                   16h
view                                                                   16h
```

### 4.2 对局域网暴露服务端口
k8s 的服务支持四种服务expose级别。[官方文档](https://kubernetes.io/docs/concepts/services-networking/service/)如下：
* **ClusterIP**: Exposes the service on a cluster-internal IP. Choosing this value makes the service only reachable from within the cluster. This is the default ServiceType.
* **NodePort**: Exposes the service on each Node’s IP at a static port (the NodePort). A ClusterIP service, to which the NodePort service will route, is automatically created. You’ll be able to contact the NodePort service, from outside the cluster, by requesting <NodeIP>:<NodePort>.
* **LoadBalancer**: Exposes the service externally using a cloud provider’s load balancer. NodePort and ClusterIP services, to which the external load balancer will route, are automatically created.
* **ExternalName**: Maps the service to the contents of the externalName field (e.g. foo.bar.example.com), by returning a CNAME record with its value. No proxying of any kind is set up. This requires version 1.7 or higher of kube-dns.

四种级别从上到下服务暴露的范围越来越大。

这里为了方便局域网内访问dashboard，我们把dashboard默认的ClusterIP更改为NodePort。

```bash
#修改yaml,添加NodePort.
kind: Service
apiVersion: v1
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard
  namespace: kube-system
spec:
  # 添加Service的type为NodePort
  type: NodePort
  ports:
    - port: 443
      targetPort: 8443
      # 添加映射到虚拟机的端口,k8s只支持30000以上的端口
      nodePort: 30001
  selector:
    k8s-app: kubernetes-dashboard
```

接下来为dahsboard配置资源图形化界面[Heapster](https://github.com/kubernetes/heapster)。至此，k8s已经基本达到我们预期的状态。

此外，k8s也提供了[ingress](https://kubernetes.io/docs/concepts/services-networking/ingress/)工具管理入口。这个方法似乎更灵活，以后用到了可以尝试一下。

### 4.3 一个关于CNI的bug

加入新节点后，daemonset无法创建。journalctl -xeu kubelet 问题如下：

May 16 17:16:04 BME311-Node3 kubelet[990]: E0516 17:16:04.475951     990 cni.go:259] Error adding network: failed to set bridge addr: "cni0" already has an IP address different from 10.244.0.1/24


> 根据 Github@[midoblgsm](https://github.com/midoblgsm) I had the same issue on Ubuntu 16.04.1 LTS (using flannel for the networking), following [#39557](https://github.com/kubernetes/kubernetes/issues/39557) resolved the issue for me: I did (on the node having the issue):

```bash
kubeadm reset
systemctl stop kubelet
systemctl stop docker
rm -rf /var/lib/cni/
rm -rf /var/lib/kubelet/*
rm -rf /etc/cni/
ifconfig cni0 down
ifconfig flannel.1 down
ifconfig docker0 down
ip link delete cni0
ip link delete flannel.1
```
then restarted docker and kubelet and did the kubeadm join dance again.

## 5. 更进一步的实践
[应用程序自检和调试](https://k8smeetup.github.io/docs/tasks/debug-application-cluster/debug-application-introspection/)

[资源管理](https://k8smeetup.github.io/docs/concepts/cluster-administration/manage-deployment/#%E4%BC%B8%E7%BC%A9%E6%82%A8%E7%9A%84%E5%BA%94%E7%94%A8)

[配置最佳实践](https://k8smeetup.github.io/docs/concepts/configuration/overview/)
